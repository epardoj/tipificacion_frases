{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESybEZDtYx3r"
   },
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download es_core_news_md\n",
    "#!pip install Unidecode\n",
    "#!pip install -U transformers\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece\n",
    "#!pip install summa\n",
    "#!python -m pip install wordcloud\n",
    "#!pip install nlpaug\n",
    "#!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSfN02BFD5Qt"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import string\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.language import Language\n",
    "from heapq import nlargest\n",
    "import re\n",
    "import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from summa.summarizer import summarize\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('omw')\n",
    "import nlpaug.augmenter.word as naw\n",
    "aumento = naw.SynonymAug(aug_src='wordnet', lang='spa')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHvOULjh9iyr"
   },
   "outputs": [],
   "source": [
    "def limpieza(documento): \n",
    "    \"\"\"Función para limpiar textos. Solo requiere que se ingrese el texto (en formato string)\n",
    "    La salida de la función es una lista con los tokens limpios\"\"\"\n",
    "    texto = unidecode.unidecode(documento)\n",
    "    texto = re.sub(\"\\d+\", ' ', texto)\n",
    "    texto = re.sub(\"\\\\s+\", ' ', texto)\n",
    "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    texto = re.sub(regex, \"\", texto)\n",
    "    documento = nlp(texto)\n",
    "    tokens_limpios = [token.lemma_ for token in documento]\n",
    "    tokens_limpios = [token for token in tokens_limpios if nlp(token)[0].is_punct == False]\n",
    "    tokens_limpios = [token for token in tokens_limpios if nlp(token)[0].is_stop == False]\n",
    "    tokens_minusculas = list(map(str.lower, tokens_limpios))\n",
    "\n",
    "    return tokens_minusculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPU0nw4rFb0Z"
   },
   "outputs": [],
   "source": [
    "def grafico_top(documento, tipo=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que genera un gráfico de las palabras más frecuentes en un texto.\n",
    "    Si la variable tipo es igual a 1 generará un gráfico de barras.\n",
    "    Si la variable tipo es igual a 2 generará una nube de palabras.\n",
    "    Por defecto genera un gráfico de barras.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = limpieza(documento)\n",
    "    frecuencia = [tokens.count(token) for token in tokens]\n",
    "    lista = dict(zip(tokens, frecuencia))\n",
    "    llaves_ordenadas = sorted(lista, key=lista.get)\n",
    "    palabras = []\n",
    "    valores = []\n",
    "\n",
    "    for i in reversed(llaves_ordenadas):\n",
    "        valor = lista.get(i)\n",
    "        largo = len(i)\n",
    "        if valor > 1:\n",
    "            if largo > 2:\n",
    "                if nlp(i)[0].pos_ != 'PROPN':\n",
    "                    palabras.append(i)\n",
    "                    valores.append(lista.get(i))\n",
    "\n",
    "    if tipo == 1:\n",
    "        fig, ax = plt.subplots(figsize=(24 ,3))\n",
    "        sns.barplot(x=palabras[0:11], y=valores[0:11])\n",
    "        plt.ylabel('Cantidad')\n",
    "        plt.xlabel('Palabras')\n",
    "        plt.ylim(0, max(valores)+1)\n",
    "        plt.title('Palabras más usadas')\n",
    "        plt.show()\n",
    "    elif tipo == 2:\n",
    "        wordcloud=WordCloud(collocations = False, background_color='white').generate(\" \".join(tokens))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DK7emhCjsPtZ"
   },
   "outputs": [],
   "source": [
    "def resumen(parrafo, porcentaje = 0.15):\n",
    "    sintesis = summarize(parrafo, language='spanish', ratio = porcentaje).replace(\"\\n\", \" \")\n",
    "    return sintesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA91LbHh1H-D"
   },
   "source": [
    "## Descriptivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uq-W6HKS1WhV"
   },
   "outputs": [],
   "source": [
    "muestra1 = pd.read_excel('entrevistas_juicio.xlsx') \n",
    "extension_casos=[]\n",
    "for a in range(0, len(muestra1.Juicio)):\n",
    "    total = [token.text.replace('\\n', ' ').lower() for token in nlp(muestra1.Juicio[a]).sents]\n",
    "    extension_casos.append(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(extension_casos))\n",
    "print(min(extension_casos))\n",
    "print(max(extension_casos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40tbvo5D3W7U"
   },
   "outputs": [],
   "source": [
    "nom_casos=['caso1', 'caso2', 'caso3', 'caso4', 'caso5', 'caso6', 'caso7', 'caso8', 'caso9', 'caso10']\n",
    "fig, ax = plt.subplots(figsize=(24 ,3))\n",
    "sns.barplot(x=nom_casos, y=extension_casos, palette='gray')\n",
    "plt.ylabel('Extensión (frases)')\n",
    "plt.xlabel('Casos')\n",
    "plt.ylim(0, max(extension_casos))\n",
    "plt.title('Extensión de casos usados en el juicio de expertos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QLyOeDS1Bhp"
   },
   "source": [
    "## Validez de contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción frases de jueces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NL9Hb2EYI0j"
   },
   "outputs": [],
   "source": [
    "juicio = pd.read_excel('Juicio_expertos.xlsx')\n",
    "juez1 = juicio.juez1\n",
    "juez2 = juicio.juez2\n",
    "juez3 = juicio.juez3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datos_juicio= juez1\n",
    "total_frases=[]\n",
    "num_de_caso=[]\n",
    "for n in range(0, len(Datos_juicio)):\n",
    "    frasedj = [token.text.replace('\\n', ' ').lower() for token in nlp(Datos_juicio[n]).sents]\n",
    "    for w in frasedj:\n",
    "        total_frases.append(w)\n",
    "        num_de_caso.append(n)\n",
    "df_juez_1=pd.DataFrame({'caso': num_de_caso, 'frases':total_frases})\n",
    "df_juez_1.to_excel('resultados/frasesjuez1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datos_juicio= juez2\n",
    "total_frases=[]\n",
    "num_de_caso=[]\n",
    "for n in range(0, len(Datos_juicio)):\n",
    "    frasedj = [token.text.replace('\\n', '  ').lower() for token in nlp(Datos_juicio[n]).sents]\n",
    "    for w in frasedj:\n",
    "        total_frases.append(w)\n",
    "        num_de_caso.append(n)\n",
    "df_juez_1=pd.DataFrame({'caso': num_de_caso, 'frases':total_frases})\n",
    "df_juez_1.to_excel('resultados/frasesjuez2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datos_juicio= juez3\n",
    "total_frases=[]\n",
    "num_de_caso=[]\n",
    "for n in range(0, len(Datos_juicio)):\n",
    "    frasedj = [token.text.replace('\\n', ' ').lower() for token in nlp(Datos_juicio[n]).sents]\n",
    "    for w in frasedj:\n",
    "        total_frases.append(w)\n",
    "        num_de_caso.append(n)\n",
    "df_juez_1=pd.DataFrame({'caso': num_de_caso, 'frases':total_frases})\n",
    "df_juez_1.to_excel('resultados/frasesjuez3.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de comparación entre jueces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparacion(lista1, lista2, porc=0.75):\n",
    "    '''\n",
    "    Función de prueba\n",
    "    '''\n",
    "    frase_similar=[]\n",
    "    caso_frase=[]\n",
    "    porc_sim=[]\n",
    "    matriz = np.empty((0, len(lista2)))\n",
    "    for m in range(len(lista1)):\n",
    "        for n in range(len(lista2)):\n",
    "            porcentaje=nlp(\" \".join(limpieza(lista1[m]))).similarity(nlp(\" \".join(limpieza(lista2[n]))))\n",
    "            porc_sim.append(round(porcentaje, 2))\n",
    "            if porcentaje >= porc:\n",
    "                frase_similar.append(lista1[m])\n",
    "                caso_frase.append(m)\n",
    "        matriz=np.append(matriz, np.array([porc_sim]), axis=0)\n",
    "        porc_sim=[]\n",
    "    return matriz, caso_frase, frase_similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syHmeqLeavX8"
   },
   "outputs": [],
   "source": [
    "def similitud(a , b, porc=0.75):\n",
    "  \"\"\"\n",
    "  La función genera una matríz de similitud de frases contenidas en dos párrafos. Estos deben estar contenidos en DataFrames.\n",
    "  Requiere 3 parámetros:\n",
    "  a = primer párrafo\n",
    "  b = segundo párrafo\n",
    "  porc = porcentaje de similitud mínima aceptada. Por defecto extrae el 75%.\n",
    "  La función arroja tres resultados:\n",
    "  1. lista_df: Una lista de matrices con los resultados de las comparaciones (valores entre 0 y 1).\n",
    "  2. lista_frases: Una lista de las frases que tienen un porcentaje de similitud igual o mayor al establecido en porc.\n",
    "  3. lista_caso: Una lista de los casos que contienen las frases obtenidas en la lista_frases.\n",
    "  \"\"\"\n",
    "    lista_df=[]\n",
    "    lista_frases=[]\n",
    "    lista_caso=[]\n",
    "    for n in range(0, len(a)):\n",
    "    caso1j1= a[n]\n",
    "    caso1j2= b[n]\n",
    "    frasec1 = [token.text.replace('\\n', ' ').lower() for token in nlp(caso1j1).sents]\n",
    "    frasec2 = [token.text.replace('\\n', ' ').lower() for token in nlp(caso1j2).sents] \n",
    "    matriz = np.empty((0, len(frasec2)))\n",
    "    lista_porc = []\n",
    "    for i in range(0, len(frasec1)):\n",
    "        for j in range(0, len(frasec2)):\n",
    "        porcentaje=nlp(\" \".join(limpieza(frasec1[i]))).similarity(nlp(\" \".join(limpieza(frasec2[j]))))\n",
    "        if porcentaje >= porc:\n",
    "            if frasec1[i] not in lista_frases:\n",
    "                lista_frases.append(frasec1[i])\n",
    "                lista_caso.append(n)\n",
    "        lista_porc.append(round(porcentaje, 2))\n",
    "        matriz=np.append(matriz, np.array([lista_porc]), axis=0)\n",
    "        lista_porc = []\n",
    "    resultados = pd.DataFrame(matriz)\n",
    "    lista_df.append(resultados)\n",
    "\n",
    "    return lista_df, lista_frases, lista_caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpPSjbLRiwFj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "listaresultados, listafrases, listacasos = similitud(juez1,juez2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9znfosgnxZAG"
   },
   "outputs": [],
   "source": [
    "listaresultados2, listafrases2, listacasos2 = similitud(juez1,juez3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8fx9l0YhXqt"
   },
   "outputs": [],
   "source": [
    "listaresultados3, listafrases3, listacasos3 = similitud(juez2,juez3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juez1y2 = pd.ExcelWriter('resultados/juez1y2.xlsx')\n",
    "largo=len(listaresultados)\n",
    "for i in range(0, largo):\n",
    "    listaresultados[i].to_excel(juez1y2, sheet_name=str(i))\n",
    "juez1y2.save()\n",
    "juez1y2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juez1y3 = pd.ExcelWriter('resultados/juez1y3.xlsx')\n",
    "largo=len(listaresultados2)\n",
    "for i in range(0, largo):\n",
    "    listaresultados2[i].to_excel(juez1y3, sheet_name=str(i))\n",
    "juez1y3.save()\n",
    "juez1y3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juez2y3 = pd.ExcelWriter('resultados/juez2y3.xlsx')\n",
    "largo=len(listaresultados3)\n",
    "for i in range(0, largo):\n",
    "    listaresultados3[i].to_excel(juez2y3, sheet_name=str(i))\n",
    "juez2y3.save()\n",
    "juez2y3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChOWpK_cjCJC"
   },
   "outputs": [],
   "source": [
    "TotalFrases = listafrases+listafrases2+listafrases3\n",
    "TotalCasos = listacasos+listacasos2+listacasos3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMXP5b26msoL"
   },
   "outputs": [],
   "source": [
    "concordancia = pd.DataFrame({'caso': TotalCasos, 'frases':TotalFrases})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgxajENVnNKn"
   },
   "outputs": [],
   "source": [
    "concordancia.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordancia.to_excel('resultados/concordancia.xlsx')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzTBaklIpUVm"
   },
   "outputs": [],
   "source": [
    "Datos_juicio= muestra1.Juicio\n",
    "total_frases=[]\n",
    "num_de_caso=[]\n",
    "for n in range(0, len(Datos_juicio)):\n",
    "    frasedj = [token.text.replace('\\n', ' ').lower() for token in nlp(Datos_juicio[n]).sents]\n",
    "    for w in frasedj:\n",
    "        total_frases.append(w)\n",
    "        num_de_caso.append(n)\n",
    "frases_juicio_original=pd.DataFrame({'caso': num_de_caso, 'frases':total_frases})\n",
    "frases_juicio_original.to_excel('resultados/frases_juicio_original.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etiquetado de casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordancia = pd.read_excel('resultados/concordancia.xlsx')\n",
    "JuicioOriginal = pd.read_excel('resultados/frases_juicio_original.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concordancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_caso=[]\n",
    "frases=[]\n",
    "relevancia=[]\n",
    "for caso in range(0,10):\n",
    "    base1=JuicioOriginal[JuicioOriginal['caso']== caso]\n",
    "    base2=concordancia[concordancia['caso'] == caso]\n",
    "    for frase in base1['frases']:\n",
    "        for juicio in base2['frases']:\n",
    "            porcentaje=nlp(\" \".join(limpieza(frase))).similarity(nlp(\" \".join(limpieza(juicio))))\n",
    "            if porcentaje >= 0.75:\n",
    "                relevancia.append(1)\n",
    "                break\n",
    "        num_caso.append(caso)\n",
    "        frases.append(frase)\n",
    "        if len(frases) != len(relevancia):\n",
    "            relevancia.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion = pd.DataFrame({'caso':num_caso,'frases':frases,'relevancia':relevancia})\n",
    "clasificacion.to_excel('resultados/frasesclasificadas1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion = pd.read_excel('resultados/frasesclasificadas1.xlsx')\n",
    "clasificacion.groupby('relevancia').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumentando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caso_aum=[]\n",
    "frase_aum=[]\n",
    "relevancia_aum=[]\n",
    "for caso, frase, relevancia in zip(list(clasificacion['caso']), list(clasificacion['frases']), \n",
    "                                   list(clasificacion['relevancia'])):\n",
    "    texto_aumentado= aumento.augment(frase, n=6) #libreria nlpaug\n",
    "    for a in texto_aumentado:\n",
    "        caso_aum.append(caso)\n",
    "        frase_aum.append(a)\n",
    "        relevancia_aum.append(relevancia)\n",
    "clasificacion_aum = pd.DataFrame({'caso':caso_aum, 'frases': frase_aum, 'relevancia': relevancia_aum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final = pd.concat([clasificacion, clasificacion_aum], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final.to_excel('resultados/clasificacion_final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final = pd.read_excel('resultados/clasificacion_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final.groupby('relevancia').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(clasificacion_final.shape)\n",
    "round(clasificacion_final['relevancia'].value_counts()/clasificacion_final.shape[0],3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final['texto_limpio'] = clasificacion_final['frases'].apply(limpieza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final['texto_limpio'] = clasificacion_final['texto_limpio'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final.to_excel('resultados/clasificacion_final_ext.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final = pd.read_excel('resultados/clasificacion_final_ext.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_final.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=clasificacion_final.texto_limpio\n",
    "y=clasificacion_final.relevancia\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "print(X_train_dtm.shape, X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred_class)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion)\n",
    "plt.title('Matriz de confusión modelo Naive Bayes')\n",
    "plt.annotate(confusion[0,0], (0,0))\n",
    "plt.annotate(confusion[0,1], (1,0), color = 'white')\n",
    "plt.annotate(confusion[1,0], (0,1), color = 'white')\n",
    "plt.annotate(confusion[1,1], (1,1))\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "ax.set_yticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "plt.xlabel('Valores predichos')\n",
    "plt.ylabel('Valores actuales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte = classification_report(y_test, y_pred_class, target_names=['Irrelevante','Relevante'], output_dict=True)\n",
    "df_reporte=pd.DataFrame(reporte).transpose()\n",
    "df_reporte.to_excel('reporte_clasificacion_NB.xlsx')\n",
    "df_reporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC(class_weight='balanced')\n",
    "%time classifier.fit(X_train_dtm, y_train)\n",
    "y_pred_class_svc = classifier.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_svc = confusion_matrix(y_test, y_pred_class_svc)\n",
    "print(confusion_svc)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion_svc)\n",
    "plt.title('Matriz de confusión modelo SVM')\n",
    "plt.annotate(confusion_svc[0,0], (0,0))\n",
    "plt.annotate(confusion_svc[0,1], (1,0), color = 'white')\n",
    "plt.annotate(confusion_svc[1,0], (0,1), color = 'white')\n",
    "plt.annotate(confusion_svc[1,1], (1,1))\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "ax.set_yticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "plt.xlabel('Valores predichos')\n",
    "plt.ylabel('Valores actuales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification Report\\n')\n",
    "reporte=classification_report(y_test, y_pred_class_svc, target_names=['Irrelevante','Relevante'], output_dict=True)\n",
    "df_reporte=pd.DataFrame(reporte).transpose()\n",
    "df_reporte.to_excel('reporte_clasificacion_SVM.xlsx')\n",
    "df_reporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PruebaModelo = pd.read_excel('resultados/prueba_prediccion.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases=PruebaModelo.oraciones\n",
    "datos=vect.transform(frases)\n",
    "prediccionnb=nb.predict(datos)\n",
    "prediccionsvc=classifier.predict(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionpruebaNB = confusion_matrix(PruebaModelo.Relevancia, prediccionnb)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusionpruebaNB)\n",
    "plt.title('Matriz de confusión prueba Naive Bayes')\n",
    "plt.annotate(confusionpruebaNB[0,0], (0,0), color = 'white')\n",
    "plt.annotate(confusionpruebaNB[0,1], (1,0), color = 'white')\n",
    "plt.annotate(confusionpruebaNB[1,0], (0,1))\n",
    "plt.annotate(confusionpruebaNB[1,1], (1,1))\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "ax.set_yticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "plt.xlabel('Valores predichos')\n",
    "plt.ylabel('Valores actuales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportepruebaNB = classification_report(PruebaModelo.Relevancia, prediccionnb, target_names=['Irrelevante','Relevante'], output_dict=True)\n",
    "df_reporte_NB=pd.DataFrame(reportepruebaNB).transpose()\n",
    "df_reporte_NB.to_excel('reporte_prueba_NB.xlsx')\n",
    "df_reporte_NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionpruebaSVM = confusion_matrix(PruebaModelo.Relevancia, prediccionsvc)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusionpruebaSVM)\n",
    "plt.title('Matriz de confusión prueba SVM')\n",
    "plt.annotate(confusionpruebaSVM[0,0], (0,0), color = 'white')\n",
    "plt.annotate(confusionpruebaSVM[0,1], (1,0), color = 'white')\n",
    "plt.annotate(confusionpruebaSVM[1,0], (0,1))\n",
    "plt.annotate(confusionpruebaSVM[1,1], (1,1), color = 'white')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "ax.set_yticklabels([''] + ['Irrelevante', 'Relevante'])\n",
    "plt.xlabel('Valores predichos')\n",
    "plt.ylabel('Valores actuales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportepruebaSVM = classification_report(PruebaModelo.Relevancia, prediccionsvc, target_names=['Irrelevante','Relevante'], output_dict=True)\n",
    "df_reporte_SVM=pd.DataFrame(reportepruebaSVM).transpose()\n",
    "df_reporte_SVM.to_excel('reporte_prueba_SVM.xlsx')\n",
    "df_reporte_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PruebaModelo['prediccionNB'] = list(prediccionnb)\n",
    "PruebaModelo['prediccionSVC'] = list(prediccionsvc)\n",
    "PruebaModelo['validacionNB']  = [1 if PruebaModelo.Relevancia[i] == PruebaModelo.prediccionNB[i] else 0 for i in PruebaModelo.index]\n",
    "PruebaModelo['validacionSVC']  = [1 if PruebaModelo.Relevancia[i] == PruebaModelo.prediccionSVC[i] else 0 for i in PruebaModelo.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El modelo Naive Bayes obtuvo un',round((PruebaModelo.validacionNB.sum()/len(PruebaModelo.validacionNB))*100,2),\"% de precisión\")\n",
    "print('El modelo SVC obtuvo un',round((PruebaModelo.validacionSVC.sum()/len(PruebaModelo.validacionNB))*100,2),\"% de precisión\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nube de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_irrelevantes = clasificacion_final[clasificacion_final['relevancia'] == 0]\n",
    "casos_relevantes = clasificacion_final[clasificacion_final['relevancia'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_irrelevantes=list(casos_irrelevantes['frases'])\n",
    "frases_irrelevantes=\" \".join(frases_irrelevantes)\n",
    "grafico_top(frases_irrelevantes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_relevantes=list(casos_relevantes['frases'])\n",
    "frases_relevantes=\" \".join(frases_relevantes)\n",
    "grafico_top(frases_relevantes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistas = pd.read_excel('Corpus2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevista_prueba = [token.text.replace('\\n', ' ').lower() for token in nlp(entrevistas['Motivo de consulta'][1]).sents]\n",
    "prueba_prediccion = pd.DataFrame({'oraciones':entrevista_prueba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_prediccion.to_excel('resultados/prueba_prediccion.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_prueba = vect.transform(entrevista_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_vector=nb.predict(vector_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado=pd.DataFrame({'frases':entrevista_prueba, 'relevancia':prediccion_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado.groupby('relevancia').size()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
